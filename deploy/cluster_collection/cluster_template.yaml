kind: RayCluster
apiVersion: cluster.ray.io/v1
metadata:
  name: {{ cluster_name }}
  labels:
      # allows me to return name of service that Ray operator creates
      odh-ray-cluster-service: {{ cluster_name }}-ray-head
  ownerReferences:
    - apiVersion: v1
      kind: pod
      name: {{ jupyter_pod_name }}
      uid: {{ jupyter_pod_uid }}
      controller: true
      blockOwnerDeletion: true
spec:
  # we can parameterize this when we fix the JH launcher json/jinja bug
  maxWorkers: 3
  # The autoscaler will scale up the cluster faster with higher upscaling speed.
  # E.g., if the task requires adding more nodes then autoscaler will gradually
  # scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
  # This number should be > 0.
  upscalingSpeed: 1.0
  # If a node is idle for this many minutes, it will be removed.
  idleTimeoutMinutes: 5
  # Specify the pod type for the ray head node (as configured below).
  headPodType: head-node
  # Specify the allowed pod types for this ray cluster and the resources they provide.
  podTypes:
  - name: head-node
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        generateName: ray-cluster-{{ cluster_name }}-head-
      spec:
        restartPolicy: Never
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: quay.io/thoth-station/ray-ml-worker:v0.2.1
          # Do not change this command - it keeps the pod alive until it is explicitly killed.
          command: ["/bin/bash", "-c", "--"]
          args: ['trap : TERM INT; sleep infinity & wait;']
          ports:
          - containerPort: 6379  # Redis port for Ray <= 1.10.0. GCS server port for Ray >= 1.11.0.
          - containerPort: 10001  # Used by Ray Client
          - containerPort: 8265  # Used by Ray Dashboard
          - containerPort: 8000 # Used by Ray Serve
          - containerPort: 8080 # Use by monitroing
          # This volume allocates shared memory for Ray to use for plasma
          env:
          # defining HOME is part of a workaround for:
          # https://github.com/ray-project/ray/issues/14155
          - name: HOME
            value: '/home'
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          resources:
            requests:
              cpu: {{ cpu_request }}
              memory: {{ memory_request }}
              ephemeral-storage: 1Gi
            limits:
              cpu: {{ cpu_limit }}
              # The maximum memory that this pod is allowed to use. The
              # limit will be detected by ray and split to use 10% for
              # redis, 30% for the shared memory object store, and the
              # rest for application memory. If this limit is not set and
              # the object store size is not set manually, ray will
              # allocate a very large object store in each pod that may
              # cause problems for other pods.
              memory: {{ memory_limit }}
              nvidia.com/gpu: {{ gpu_limit }}
  - name: worker-nodes
    # we can parameterize this when we fix the JH launcher json/jinja bug
    minWorkers: 1
    maxWorkers: 3
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        # Automatically generates a name for the pod with this prefix.
        generateName: 'ray-cluster-example-worker-'
      spec:
        restartPolicy: Never
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: quay.io/thoth-station/ray-ml-worker:v0.2.1
          command: ["/bin/bash", "-c", "--"]
          args: ["trap : TERM INT; sleep infinity & wait;"]
          env:
          - name: HOME
            value: '/home'
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          resources:
            requests:
              cpu: {{ cpu_request }}
              memory: {{ memory_request }}
            limits:
              cpu: {{ cpu_limit }}
              memory: {{ memory_limit }}
              nvidia.com/gpu: {{ gpu_limit }}
  # Commands to start Ray on the head node. You don't need to change this.
  # Note dashboard-host is set to 0.0.0.0 so that Kubernetes can port forward.
  
  headServicePorts:
      - name: monitoring
        port: 8080
        targetPort: 8080
  headStartRayCommands:
      - cd /home/ray; pipenv run ray stop
      - ulimit -n 65536; cd /home/ray; pipenv run ray start --head --metrics-export-port=8080 --port=6379 --object-manager-port=8076 --dashboard-host=0.0.0.0
  # Commands to start Ray on worker nodes. You don't need to change this.
  workerStartRayCommands:
      - cd /home/ray; pipenv run ray stop
      - ulimit -n 65536; cd /home/ray; pipenv run ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
